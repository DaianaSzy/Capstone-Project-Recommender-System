{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify and set the filepath structure for the project data to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import json\n",
    "\n",
    "# Optional: to suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introductionary examples for os library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate how os.getcwd(), os.path.join(), os.path.exists() work\n",
    "\n",
    "path = os.path.join(os.getcwd(),\"..\", 'data')\n",
    "print(f\"The path {path} exists:\\n{os.path.exists(path)}\\n\") # returns True if the folder or file exists\n",
    "\n",
    "path = os.path.join(path, '_original', 'All_Beauty.jsonl.gz')\n",
    "print(f\"The path {path} exists:\\n{os.path.exists(path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate how os.path.pardir and os.path.abspath() work\n",
    "\n",
    "# get the relative path of the parent folder of the working directory\n",
    "path = os.path.join(os.getcwd(), os.pardir)\n",
    "print(path)\n",
    "\n",
    "# and return the absolute path of the parent directory\n",
    "path = os.path.abspath(path)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create project folder structure and download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current notebook working directory (\"./notebooks\")  for the \"./../data\" folder and it's subolders to be created:\n",
    "path = os.path.join(os.getcwd(), os.pardir) # get the relative path of the parent folder of the working directory\n",
    "path = os.path.abspath(path) # and return the absolute path of the parent directory for the data folder\n",
    "path = os.path.join(path, 'data') # path variable for \"data\" folder within the local github project on your machine\n",
    "\n",
    "# Check if the structure within data folder is existing:\n",
    "if os.path.exists(os.path.join(path, '_original')) == False:\n",
    "    # create the folder\n",
    "    os.mkdir(os.path.join(path, '_original'))\n",
    "    # download the data from the web (XXX_ToBeTested)\n",
    "    url_user = 'https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/All_Beauty.jsonl.gz'\n",
    "    url_meta = 'https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/meta_categories/meta_All_Beauty.jsonl.gz'\n",
    "    url_BERT = 'https://drive.google.com/file/d/1a_HQWlphn-wqm6MD5IDPTuzY_-Q7bw6A'\n",
    "\n",
    "    response_user = requests.get(url_user)\n",
    "    response_meta = requests.get(url_meta)\n",
    "    response_BERT = requests.get(url_BERT)\n",
    "\n",
    "    if response_user.status_code == 200:\n",
    "        with open(os.path.join(path, '_original','All_Beauty.jsonl.gz'), 'wb') as file:\n",
    "            file.write(response_user.content)\n",
    "        print(f\" File 'All_Beaty.json.gz' dowloaded sucessfully\")\n",
    "    else:\n",
    "        print(f\" Failed to download 'All_Beaty.json.gz'. \\nPlease download manually to './data/_original' folder.\")\n",
    "\n",
    "    if response_meta.status_code == 200:\n",
    "        with open(os.path.join(path, '_original', 'meta_All_Beauty.jsonl.gz'), 'wb') as file:\n",
    "            file.write(response_meta.content)\n",
    "        print(f\" File 'meta_All_Beaty.json.gz' dowloaded sucessfully\")\n",
    "    else:\n",
    "        print(f\" Failed to download 'meta_All_Beaty.json.gz'. \\nPlease download manually to './data/_original' folder.\")\n",
    "    \n",
    "    if response_BERT.status_code == 200:\n",
    "        with open(os.path.join(path, '_original', 'df_user_embeddings_BERT.csv.gz'), 'wb') as file:\n",
    "            file.write(response_BERT.content)\n",
    "        print(f\" File 'df_user_embeddings_BERT.csv.gz' dowloaded sucessfully\")\n",
    "    else:\n",
    "        print(f\" Failed to download 'df_user_embeddings_BERT.csv.gz'. \\nPlease download manually to './data/_original' folder.\")\n",
    "\n",
    "elif os.path.exists(os.path.join(path, 'json_files')) == False:    \n",
    "    source = os.path.join(path, '_original')\n",
    "    destination = os.path.join(path, 'json_files')\n",
    "    \n",
    "    # create the folder\n",
    "    os.mkdir(destination)\n",
    "    ExtractAndMoveUnzipped(source, destination)\n",
    "    \n",
    "elif os.path.exists(os.path.join(path, 'csv_transformed')) == False:\n",
    "    os.mkdir(os.path.join(path, 'csv_transformed'))\n",
    "\n",
    "elif os.path.exists(os.path.join(path, 'data_clean')) == False:\n",
    "    os.mkdir(os.path.join(path, 'data_clean'))\n",
    "\n",
    "elif os.path.exists(os.path.join(path, 'json_normalized')) == False:\n",
    "    os.mkdir(os.path.join(path, 'json_normalized'))\n",
    "\n",
    "elif os.path.exists(os.path.join(path, 'embeddings_output')) == False:\n",
    "    source = os.path.join(path, '_original')\n",
    "    destination = os.path.join(path, 'embeddings_output')\n",
    "\n",
    "    # create the folder\n",
    "    os.mkdir(destination)\n",
    "    ExtractAndMoveUnzipped(source, destination)\n",
    "\n",
    "elif os.path.exists(os.path.join(path, 'embeddings_dim_reduction')) == False:\n",
    "    os.mkdir(os.path.join(path, 'embeddings_dim_reduction'))\n",
    "\n",
    "elif os.path.exists(os.path.join(path, 'embeddings_dim_reduction')) == False:\n",
    "    os.mkdir(os.path.join(path, 'embeddings_dim_reduction'))\n",
    "\n",
    "elif os.path.exists(os.path.join(path, 'embeddings_dim_reduction')) == False:\n",
    "    os.mkdir(os.path.join(path, 'embeddings_dim_reduction'))\n",
    "\n",
    "elif os.path.exists(os.path.join(path, 'text_analysis')) == False:\n",
    "    os.mkdir(os.path.join(path, 'text_analysis'))\n",
    "    os.mkdir(os.path.join(path, 'text_analysis', 'user_vectors'))\n",
    "    os.mkdir(os.path.join(path, 'text_analysis', 'product_vectors'))\n",
    "   \n",
    "elif os.path.exists(os.path.join(path, 'embeddings_output')) == False:\n",
    "    os.mkdir(os.path.join(path, 'embeddings_output'))\n",
    "\n",
    "elif os.path.exists(os.path.join(path, 'embeddings_dim_reduction')) == False:\n",
    "    os.mkdir(os.path.join(path, 'embeddings_dim_reduction'))\n",
    "\n",
    "elif os.path.exists(os.path.join(path, 'cos_similarity')) == False:\n",
    "    os.mkdir(os.path.join(path, 'cos_similarity')) \n",
    "\n",
    "\n",
    "def ExtractAndMoveUnzipped(source, destination):\n",
    "    # check if unzip and shutil packages are installed:\n",
    "    pip_list = os.popen('pip list').read().strip() #pip list all installed packages and remove spaces\n",
    "    #print(f\"pip list: {pip_list}\")\n",
    "    Package = list(pip_list.split(\"\\n\")) #split by end of line\n",
    "    #print(f\"Package: {Package}\")\n",
    "\n",
    "    c = 0\n",
    "    for i in Package:\n",
    "        if \"gzip\" in i or \"shutil\" in i:\n",
    "            c = c + 1\n",
    "\n",
    "    if c == 2:\n",
    "        # unzip downloaded json files from _original into json_files\n",
    "        for filename in os.listdir(source):\n",
    "            if filename.endswith('.gz'): \n",
    "                with gzip.open(filename, 'rb') as f_in:\n",
    "                    with open(filename[:-3], 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "                        shutil.move(os.path.join(source, f_out), destination)\n",
    "    else:\n",
    "        print(\"Please extract the .gz files in './data/_original' and \\nmove the JSON files to './data/json_files' and \\move the embedding CSV files to '.data/embeddings_output' \\nfor this notebook to work properly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a flattend CSV from the downloaded JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the nested JSON datasets of the items descpritions and users ratings into flattend CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the relative path of the parent folder of the working directory\n",
    "# and return the absolute path of the parent directory for the data folder\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) \n",
    "path = os.path.join(path, 'data') # path variable for \"data\" folder within the local github project on your machine\n",
    "\n",
    "if os.path.exists(os.path.join(path, 'json_normalized', 'normalized_user.csv')) == False:\n",
    "    # Flatten nested json file of 'user' data\n",
    "    json_file_user = os.path.join(path, \"json_files\", \"All_Beauty.jsonl\") #'.\\..\\data\\json_files\\All_Beauty.jsonl'\n",
    "    nested_data = []\n",
    "    with open(json_file_user, 'r') as file:\n",
    "        for line in file:\n",
    "            nested_data.append(json.loads(line))\n",
    "\n",
    "    df_user = json_normalize(nested_data)\n",
    "    df_user.to_csv(r'.\\..\\data\\json_normalized\\normalized_user.csv', index=False)\n",
    "    \n",
    "    print(\"Nested JSON file has been flattened and saved as \\n'.\\data\\json_normalized\\normalized_user.csv'.\")\n",
    "\n",
    "# \n",
    "elif os.path.exists(os.path.join(path, 'csv_transformed', 'meta.csv')) == False:\n",
    "    json_file_meta = os.path.join(path, \"json_files\", \"meta_All_Beauty.jsonl\") #'.\\..\\data\\json_files\\meta_All_Beauty.jsonl'\n",
    "    df_meta = pd.read_json(json_file_meta, lines=True)\n",
    "    # Specify the path for the CSV:\n",
    "    csv_meta = r'.\\..\\data\\csv_transformed\\meta.csv'\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df_meta.to_csv(csv_meta, index=False)\n",
    "    \n",
    "    print(f\"JSON file has been converted to CSV and saved as '{csv_meta}'.\")\n",
    "\n",
    "else:\n",
    "    print(\"Most likely the corresponding folder structure and csv file already exists.\\nIn case it does not please repeat the previous steps or adapt the folder structure manually\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
